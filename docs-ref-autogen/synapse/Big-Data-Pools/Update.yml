### YamlMime:RESTOperation
uid: management.azure.com.synapse.bigdatapools.update
name: Update
service: Synapse
groupName: Big Data Pools
apiVersion: 2021-06-01-preview
summary: "Update a Big Data pool.  \nPatch a Big Data pool."
consumes:
- application/json
produces:
- application/json
paths:
- content: PATCH https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Synapse/workspaces/{workspaceName}/bigDataPools/{bigDataPoolName}?api-version=2021-06-01-preview
uriParameters:
- name: subscriptionId
  in: path
  isRequired: true
  description: The ID of the target subscription.
  types:
  - uid: string
- name: resourceGroupName
  in: path
  isRequired: true
  description: The name of the resource group. The name is case insensitive.
  types:
  - uid: string
- name: workspaceName
  in: path
  isRequired: true
  description: The name of the workspace.
  types:
  - uid: string
- name: bigDataPoolName
  in: path
  isRequired: true
  description: Big Data pool name
  types:
  - uid: string
- name: api-version
  in: query
  isRequired: true
  description: The API version to use for this operation.
  types:
  - uid: string
responses:
- name: 200 OK
  description: ''
  types:
  - uid: BigDataPoolResourceInfo
- name: Other Status Codes
  description: ''
  types:
  - uid: ErrorResponse
requestBody:
- name: default
  parameters:
  - name: tags
    in: body
    description: Updated tags for the Big Data pool
    types:
    - uid: object
      isDictionary: true
      additionalTypes:
      - uid: string
      - uid: string
requestHeader: []
definitions:
- name: BigDataPoolResourceInfo
  description: Big Data pool
  kind: object
  properties:
  - name: properties.provisioningState
    description: The state of the Big Data pool.
    types:
    - uid: string
  - name: properties.autoScale
    description: "Spark pool auto-scaling properties  \nAuto-scaling properties"
    types:
    - uid: AutoScaleProperties
  - name: properties.creationDate
    isReadyOnly: true
    description: The time when the Big Data pool was created.
    types:
    - uid: string
  - name: properties.autoPause
    description: "Spark pool auto-pausing properties  \nAuto-pausing properties"
    types:
    - uid: AutoPauseProperties
  - name: properties.isComputeIsolationEnabled
    description: Whether compute isolation is required or not.
    types:
    - uid: boolean
  - name: properties.sessionLevelPackagesEnabled
    description: Whether session level packages enabled.
    types:
    - uid: boolean
  - name: properties.cacheSize
    description: The cache size
    types:
    - uid: integer
  - name: properties.dynamicExecutorAllocation
    description: Dynamic Executor Allocation
    types:
    - uid: DynamicExecutorAllocation
  - name: properties.sparkEventsFolder
    description: The Spark events folder
    types:
    - uid: string
  - name: properties.nodeCount
    description: The number of nodes in the Big Data pool.
    types:
    - uid: integer
  - name: properties.libraryRequirements
    description: "Spark pool library version requirements  \nLibrary version requirements"
    types:
    - uid: LibraryRequirements
  - name: properties.customLibraries
    description: List of custom libraries/packages associated with the spark pool.
    types:
    - uid: LibraryInfo
      isArray: true
  - name: properties.sparkConfigProperties
    description: "Spark pool Config Properties  \nSpark configuration file to specify additional properties"
    types:
    - uid: SparkConfigProperties
  - name: properties.sparkVersion
    description: The Apache Spark version.
    types:
    - uid: string
  - name: properties.defaultSparkLogFolder
    description: The default folder where Spark logs will be written.
    types:
    - uid: string
  - name: properties.nodeSize
    description: The level of compute power that each node in the Big Data pool has.
    types:
    - uid: NodeSize
  - name: properties.nodeSizeFamily
    description: The kind of nodes that the Big Data pool provides.
    types:
    - uid: NodeSizeFamily
  - name: properties.lastSucceededTimestamp
    isReadyOnly: true
    description: The time when the Big Data pool was updated successfully.
    types:
    - uid: string
  - name: tags
    description: Resource tags.
    types:
    - uid: object
      isDictionary: true
      additionalTypes:
      - uid: string
      - uid: string
  - name: location
    description: The geo-location where the resource lives
    types:
    - uid: string
  - name: id
    isReadyOnly: true
    description: Fully qualified resource ID for the resource. Ex - /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}
    types:
    - uid: string
  - name: name
    isReadyOnly: true
    description: The name of the resource
    types:
    - uid: string
  - name: type
    isReadyOnly: true
    description: The type of the resource. E.g. "Microsoft.Compute/virtualMachines" or "Microsoft.Storage/storageAccounts"
    types:
    - uid: string
- name: ErrorResponse
  description: Error response
  kind: object
  properties:
  - name: error
    description: The error object.
    types:
    - uid: ErrorDetail
- name: BigDataPoolPatchInfo
  description: Patch for a Big Data pool
  kind: object
  properties:
  - name: tags
    description: Updated tags for the Big Data pool
    types:
    - uid: object
      isDictionary: true
      additionalTypes:
      - uid: string
      - uid: string
- name: AutoScaleProperties
  description: Spark pool auto-scaling properties
  kind: object
  properties:
  - name: minNodeCount
    description: The minimum number of nodes the Big Data pool can support.
    types:
    - uid: integer
  - name: enabled
    description: Whether automatic scaling is enabled for the Big Data pool.
    types:
    - uid: boolean
  - name: maxNodeCount
    description: The maximum number of nodes the Big Data pool can support.
    types:
    - uid: integer
- name: AutoPauseProperties
  description: Spark pool auto-pausing properties
  kind: object
  properties:
  - name: delayInMinutes
    description: Number of minutes of idle time before the Big Data pool is automatically paused.
    types:
    - uid: integer
  - name: enabled
    description: Whether auto-pausing is enabled for the Big Data pool.
    types:
    - uid: boolean
- name: DynamicExecutorAllocation
  description: Dynamic Executor Allocation Properties
  kind: object
  properties:
  - name: enabled
    description: Indicates whether Dynamic Executor Allocation is enabled or not.
    types:
    - uid: boolean
  - name: minExecutors
    description: The minimum number of executors alloted
    types:
    - uid: integer
  - name: maxExecutors
    description: The maximum number of executors alloted
    types:
    - uid: integer
- name: LibraryRequirements
  description: Spark pool library version requirements
  kind: object
  properties:
  - name: time
    isReadyOnly: true
    description: The last update time of the library requirements file.
    types:
    - uid: string
  - name: content
    description: The library requirements.
    types:
    - uid: string
  - name: filename
    description: The filename of the library requirements file.
    types:
    - uid: string
- name: LibraryInfo
  description: Information about a library/package created at the workspace level.
  kind: object
  properties:
  - name: name
    description: Name of the library.
    types:
    - uid: string
  - name: path
    description: Storage blob path of library.
    types:
    - uid: string
  - name: containerName
    description: Storage blob container name.
    types:
    - uid: string
  - name: uploadedTimestamp
    description: The last update time of the library.
    types:
    - uid: string
  - name: type
    description: Type of the library.
    types:
    - uid: string
  - name: provisioningStatus
    isReadyOnly: true
    description: Provisioning status of the library/package.
    types:
    - uid: string
  - name: creatorId
    isReadyOnly: true
    description: Creator Id of the library/package.
    types:
    - uid: string
- name: SparkConfigProperties
  description: Spark pool Config Properties
  kind: object
  properties:
  - name: time
    isReadyOnly: true
    description: The last update time of the spark config properties file.
    types:
    - uid: string
  - name: content
    description: The spark config properties.
    types:
    - uid: string
  - name: filename
    description: The filename of the spark config properties file.
    types:
    - uid: string
  - name: configurationType
    description: The type of the spark config properties file.
    types:
    - uid: ConfigurationType
- name: NodeSize
  description: The level of compute power that each node in the Big Data pool has.
  kind: enum
  properties:
  - name: None
    types:
    - uid: string
  - name: Small
    types:
    - uid: string
  - name: Medium
    types:
    - uid: string
  - name: Large
    types:
    - uid: string
  - name: XLarge
    types:
    - uid: string
  - name: XXLarge
    types:
    - uid: string
  - name: XXXLarge
    types:
    - uid: string
- name: NodeSizeFamily
  description: The kind of nodes that the Big Data pool provides.
  kind: enum
  properties:
  - name: None
    types:
    - uid: string
  - name: MemoryOptimized
    types:
    - uid: string
  - name: HardwareAcceleratedFPGA
    types:
    - uid: string
  - name: HardwareAcceleratedGPU
    types:
    - uid: string
- name: ErrorDetail
  description: The error detail.
  kind: object
  properties:
  - name: code
    isReadyOnly: true
    description: The error code.
    types:
    - uid: string
  - name: message
    isReadyOnly: true
    description: The error message.
    types:
    - uid: string
  - name: target
    isReadyOnly: true
    description: The error target.
    types:
    - uid: string
  - name: details
    isReadyOnly: true
    description: The error details.
    types:
    - uid: ErrorDetail
      isArray: true
  - name: additionalInfo
    isReadyOnly: true
    description: The error additional info.
    types:
    - uid: ErrorAdditionalInfo
      isArray: true
- name: ConfigurationType
  description: The type of the spark config properties file.
  kind: enum
  properties:
  - name: File
    types:
    - uid: string
  - name: Artifact
    types:
    - uid: string
- name: ErrorAdditionalInfo
  description: The resource management error additional info.
  kind: object
  properties:
  - name: type
    isReadyOnly: true
    description: The additional info type.
    types:
    - uid: string
  - name: info
    isReadyOnly: true
    description: The additional info.
    types:
    - uid: object
examples:
- name: Update a Big Data pool
  request:
    uri: PATCH https://management.azure.com/subscriptions/01234567-89ab-4def-0123-456789abcdef/resourceGroups/ExampleResourceGroup/providers/Microsoft.Synapse/workspaces/ExampleWorkspace/bigDataPools/ExamplePool?api-version=2021-06-01-preview
    body: >-
      {
        "tags": {
          "key": "value"
        }
      }
    codeTab: |+
      # [HTTP](#tab/HTTP)
      ``` http
      PATCH https://management.azure.com/subscriptions/01234567-89ab-4def-0123-456789abcdef/resourceGroups/ExampleResourceGroup/providers/Microsoft.Synapse/workspaces/ExampleWorkspace/bigDataPools/ExamplePool?api-version=2021-06-01-preview

      {
        "tags": {
          "key": "value"
        }
      }

      ```

      # [Java](#tab/Java)
      ``` java
      import com.azure.core.util.Context;
      import com.azure.resourcemanager.synapse.models.BigDataPoolResourceInfo;
      import java.util.HashMap;
      import java.util.Map;

      /** Samples for BigDataPools Update. */
      public final class Main {
          /*
           * x-ms-original-file: specification/synapse/resource-manager/Microsoft.Synapse/preview/2021-06-01-preview/examples/UpdateBigDataPool.json
           */
          /**
           * Sample code: Update a Big Data pool.
           *
           * @param manager Entry point to SynapseManager.
           */
          public static void updateABigDataPool(com.azure.resourcemanager.synapse.SynapseManager manager) {
              BigDataPoolResourceInfo resource =
                  manager
                      .bigDataPools()
                      .getWithResponse("ExampleResourceGroup", "ExampleWorkspace", "ExamplePool", Context.NONE)
                      .getValue();
              resource.update().withTags(mapOf("key", "value")).apply();
          }

          @SuppressWarnings("unchecked")
          private static <T> Map<String, T> mapOf(Object... inputs) {
              Map<String, T> map = new HashMap<>();
              for (int i = 0; i < inputs.length; i += 2) {
                  String key = (String) inputs[i];
                  T value = (T) inputs[i + 1];
                  map.put(key, value);
              }
              return map;
          }
      }

      ```
      Read this [SDK documentation](https://github.com/Azure/azure-sdk-for-java/blob/azure-resourcemanager-synapse_1.0.0-beta.6/sdk/synapse/azure-resourcemanager-synapse/README.md) on how to add the SDK to your project and authenticate.
      # [Go](#tab/Go)
      ``` go
      package armsynapse_test

      import (
      	"context"
      	"log"

      	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
      	"github.com/Azure/azure-sdk-for-go/sdk/azidentity"
      	"github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/synapse/armsynapse"
      )

      // Generated from example definition: https://github.com/Azure/azure-rest-api-specs/tree/main/specification/synapse/resource-manager/Microsoft.Synapse/preview/2021-06-01-preview/examples/UpdateBigDataPool.json
      func ExampleBigDataPoolsClient_Update() {
      	cred, err := azidentity.NewDefaultAzureCredential(nil)
      	if err != nil {
      		log.Fatalf("failed to obtain a credential: %v", err)
      	}
      	ctx := context.Background()
      	client, err := armsynapse.NewBigDataPoolsClient("01234567-89ab-4def-0123-456789abcdef", cred, nil)
      	if err != nil {
      		log.Fatalf("failed to create client: %v", err)
      	}
      	res, err := client.Update(ctx,
      		"ExampleResourceGroup",
      		"ExampleWorkspace",
      		"ExamplePool",
      		armsynapse.BigDataPoolPatchInfo{
      			Tags: map[string]*string{
      				"key": to.Ptr("value"),
      			},
      		},
      		nil)
      	if err != nil {
      		log.Fatalf("failed to finish the request: %v", err)
      	}
      	// TODO: use response item
      	_ = res
      }

      ```
      Read this [SDK documentation](https://github.com/Azure/azure-sdk-for-go/blob/sdk%2Fresourcemanager%2Fsynapse%2Farmsynapse%2Fv0.5.0/sdk/resourcemanager/synapse/armsynapse/README.md) on how to add the SDK to your project and authenticate.
      # [JavaScript](#tab/JavaScript)
      ``` js
      const { SynapseManagementClient } = require("@azure/arm-synapse");
      const { DefaultAzureCredential } = require("@azure/identity");

      /**
       * This sample demonstrates how to Patch a Big Data pool.
       *
       * @summary Patch a Big Data pool.
       * x-ms-original-file: specification/synapse/resource-manager/Microsoft.Synapse/preview/2021-06-01-preview/examples/UpdateBigDataPool.json
       */
      async function updateABigDataPool() {
        const subscriptionId = "01234567-89ab-4def-0123-456789abcdef";
        const resourceGroupName = "ExampleResourceGroup";
        const workspaceName = "ExampleWorkspace";
        const bigDataPoolName = "ExamplePool";
        const bigDataPoolPatchInfo = { tags: { key: "value" } };
        const credential = new DefaultAzureCredential();
        const client = new SynapseManagementClient(credential, subscriptionId);
        const result = await client.bigDataPools.update(
          resourceGroupName,
          workspaceName,
          bigDataPoolName,
          bigDataPoolPatchInfo
        );
        console.log(result);
      }

      updateABigDataPool().catch(console.error);

      ```
      Read this [SDK documentation](https://github.com/Azure/azure-sdk-for-js/blob/%40azure%2Farm-synapse_8.1.0-beta.1/sdk/synapse/arm-synapse/README.md) on how to add the SDK to your project and authenticate.
  responses:
  - statusCode: "200"
    body: >-
      {
        "id": "/subscriptions/01234567-89ab-4def-0123-456789abcdef/resourceGroups/ExampleResourceGroup/providers/Microsoft.Synapse/workspaces/ExampleWorkspace/bigDataPools/ExamplePool",
        "type": "Microsoft.Synapse/workspaces/bigDataPools",
        "location": "West US 2",
        "name": "ExamplePool",
        "tags": {
          "key": "value"
        },
        "properties": {
          "provisioningState": "Succeeded",
          "sparkVersion": "2.4",
          "nodeCount": 4,
          "nodeSize": "Medium",
          "nodeSizeFamily": "MemoryOptimized",
          "autoScale": {
            "enabled": true,
            "minNodeCount": 3,
            "maxNodeCount": 50
          },
          "autoPause": {
            "enabled": true,
            "delayInMinutes": 15
          },
          "creationDate": "1970-01-01T00:00:00Z",
          "sparkEventsFolder": "/events",
          "libraryRequirements": {
            "time": "1970-01-01T00:00:00Z",
            "content": "",
            "filename": "requirements.txt"
          },
          "defaultSparkLogFolder": "/logs"
        }
      }
  - statusCode: default
    body: >-
      {
        "error": {
          "code": "Error code",
          "message": "Error message"
        }
      }
security: []
metadata:
  description: >
    Learn more about Synapse service - Update a Big Data pool.

    Patch a Big Data pool.
errorCodes: []
