### YamlMime:RESTOperation
uid: management.azure.com.synapse.bigdatapools.createorupdate
name: Create Or Update
service: Synapse
groupName: Big Data Pools
apiVersion: 2021-06-01-preview
summary: "Create a Big Data pool.  \nCreate a new Big Data pool."
consumes:
- application/json
produces:
- application/json
paths:
- content: PUT https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Synapse/workspaces/{workspaceName}/bigDataPools/{bigDataPoolName}?api-version=2021-06-01-preview
- content: PUT https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Synapse/workspaces/{workspaceName}/bigDataPools/{bigDataPoolName}?api-version=2021-06-01-preview&force={force}
  isOptional: true
uriParameters:
- name: subscriptionId
  in: path
  isRequired: true
  description: The ID of the target subscription.
  types:
  - uid: string
- name: resourceGroupName
  in: path
  isRequired: true
  description: The name of the resource group. The name is case insensitive.
  types:
  - uid: string
- name: workspaceName
  in: path
  isRequired: true
  description: The name of the workspace.
  types:
  - uid: string
- name: bigDataPoolName
  in: path
  isRequired: true
  description: Big Data pool name
  types:
  - uid: string
- name: api-version
  in: query
  isRequired: true
  description: The API version to use for this operation.
  types:
  - uid: string
- name: force
  in: query
  description: Whether to stop any running jobs in the Big Data pool
  types:
  - uid: boolean
responses:
- name: 200 OK
  description: OK
  types:
  - uid: BigDataPoolResourceInfo
- name: 202 Accepted
  description: ''
  types:
  - uid: BigDataPoolResourceInfo
- name: Other Status Codes
  description: ''
  types:
  - uid: ErrorResponse
requestBody:
- name: default
  parameters:
  - name: properties.provisioningState
    in: body
    description: The state of the Big Data pool.
    types:
    - uid: string
  - name: properties.autoScale
    in: body
    description: "Spark pool auto-scaling properties  \nAuto-scaling properties"
    types:
    - uid: AutoScaleProperties
  - name: properties.autoPause
    in: body
    description: "Spark pool auto-pausing properties  \nAuto-pausing properties"
    types:
    - uid: AutoPauseProperties
  - name: properties.isComputeIsolationEnabled
    in: body
    description: Whether compute isolation is required or not.
    types:
    - uid: boolean
  - name: properties.sessionLevelPackagesEnabled
    in: body
    description: Whether session level packages enabled.
    types:
    - uid: boolean
  - name: properties.cacheSize
    in: body
    description: The cache size
    types:
    - uid: integer
  - name: properties.dynamicExecutorAllocation
    in: body
    description: Dynamic Executor Allocation
    types:
    - uid: DynamicExecutorAllocation
  - name: properties.sparkEventsFolder
    in: body
    description: The Spark events folder
    types:
    - uid: string
  - name: properties.nodeCount
    in: body
    description: The number of nodes in the Big Data pool.
    types:
    - uid: integer
  - name: properties.libraryRequirements
    in: body
    description: "Spark pool library version requirements  \nLibrary version requirements"
    types:
    - uid: LibraryRequirements
  - name: properties.customLibraries
    in: body
    description: List of custom libraries/packages associated with the spark pool.
    types:
    - uid: LibraryInfo
      isArray: true
  - name: properties.sparkConfigProperties
    in: body
    description: "Spark pool Config Properties  \nSpark configuration file to specify additional properties"
    types:
    - uid: SparkConfigProperties
  - name: properties.sparkVersion
    in: body
    description: The Apache Spark version.
    types:
    - uid: string
  - name: properties.defaultSparkLogFolder
    in: body
    description: The default folder where Spark logs will be written.
    types:
    - uid: string
  - name: properties.nodeSize
    in: body
    description: The level of compute power that each node in the Big Data pool has.
    types:
    - uid: NodeSize
  - name: properties.nodeSizeFamily
    in: body
    description: The kind of nodes that the Big Data pool provides.
    types:
    - uid: NodeSizeFamily
  - name: tags
    in: body
    description: Resource tags.
    types:
    - uid: object
      isDictionary: true
      additionalTypes:
      - uid: string
      - uid: string
  - name: location
    in: body
    isRequired: true
    description: The geo-location where the resource lives
    types:
    - uid: string
requestHeader: []
definitions:
- name: AutoScaleProperties
  description: Spark pool auto-scaling properties
  kind: object
  properties:
  - name: minNodeCount
    description: The minimum number of nodes the Big Data pool can support.
    types:
    - uid: integer
  - name: enabled
    description: Whether automatic scaling is enabled for the Big Data pool.
    types:
    - uid: boolean
  - name: maxNodeCount
    description: The maximum number of nodes the Big Data pool can support.
    types:
    - uid: integer
- name: AutoPauseProperties
  description: Spark pool auto-pausing properties
  kind: object
  properties:
  - name: delayInMinutes
    description: Number of minutes of idle time before the Big Data pool is automatically paused.
    types:
    - uid: integer
  - name: enabled
    description: Whether auto-pausing is enabled for the Big Data pool.
    types:
    - uid: boolean
- name: DynamicExecutorAllocation
  description: Dynamic Executor Allocation Properties
  kind: object
  properties:
  - name: enabled
    description: Indicates whether Dynamic Executor Allocation is enabled or not.
    types:
    - uid: boolean
  - name: minExecutors
    description: The minimum number of executors alloted
    types:
    - uid: integer
  - name: maxExecutors
    description: The maximum number of executors alloted
    types:
    - uid: integer
- name: LibraryRequirements
  description: Spark pool library version requirements
  kind: object
  properties:
  - name: time
    isReadyOnly: true
    description: The last update time of the library requirements file.
    types:
    - uid: string
  - name: content
    description: The library requirements.
    types:
    - uid: string
  - name: filename
    description: The filename of the library requirements file.
    types:
    - uid: string
- name: LibraryInfo
  description: Information about a library/package created at the workspace level.
  kind: object
  properties:
  - name: name
    description: Name of the library.
    types:
    - uid: string
  - name: path
    description: Storage blob path of library.
    types:
    - uid: string
  - name: containerName
    description: Storage blob container name.
    types:
    - uid: string
  - name: uploadedTimestamp
    description: The last update time of the library.
    types:
    - uid: string
  - name: type
    description: Type of the library.
    types:
    - uid: string
  - name: provisioningStatus
    isReadyOnly: true
    description: Provisioning status of the library/package.
    types:
    - uid: string
  - name: creatorId
    isReadyOnly: true
    description: Creator Id of the library/package.
    types:
    - uid: string
- name: SparkConfigProperties
  description: Spark pool Config Properties
  kind: object
  properties:
  - name: time
    isReadyOnly: true
    description: The last update time of the spark config properties file.
    types:
    - uid: string
  - name: content
    description: The spark config properties.
    types:
    - uid: string
  - name: filename
    description: The filename of the spark config properties file.
    types:
    - uid: string
  - name: configurationType
    description: The type of the spark config properties file.
    types:
    - uid: ConfigurationType
- name: BigDataPoolResourceInfo
  description: Big Data pool
  kind: object
  properties:
  - name: properties.provisioningState
    description: The state of the Big Data pool.
    types:
    - uid: string
  - name: properties.autoScale
    description: "Spark pool auto-scaling properties  \nAuto-scaling properties"
    types:
    - uid: AutoScaleProperties
  - name: properties.creationDate
    isReadyOnly: true
    description: The time when the Big Data pool was created.
    types:
    - uid: string
  - name: properties.autoPause
    description: "Spark pool auto-pausing properties  \nAuto-pausing properties"
    types:
    - uid: AutoPauseProperties
  - name: properties.isComputeIsolationEnabled
    description: Whether compute isolation is required or not.
    types:
    - uid: boolean
  - name: properties.sessionLevelPackagesEnabled
    description: Whether session level packages enabled.
    types:
    - uid: boolean
  - name: properties.cacheSize
    description: The cache size
    types:
    - uid: integer
  - name: properties.dynamicExecutorAllocation
    description: Dynamic Executor Allocation
    types:
    - uid: DynamicExecutorAllocation
  - name: properties.sparkEventsFolder
    description: The Spark events folder
    types:
    - uid: string
  - name: properties.nodeCount
    description: The number of nodes in the Big Data pool.
    types:
    - uid: integer
  - name: properties.libraryRequirements
    description: "Spark pool library version requirements  \nLibrary version requirements"
    types:
    - uid: LibraryRequirements
  - name: properties.customLibraries
    description: List of custom libraries/packages associated with the spark pool.
    types:
    - uid: LibraryInfo
      isArray: true
  - name: properties.sparkConfigProperties
    description: "Spark pool Config Properties  \nSpark configuration file to specify additional properties"
    types:
    - uid: SparkConfigProperties
  - name: properties.sparkVersion
    description: The Apache Spark version.
    types:
    - uid: string
  - name: properties.defaultSparkLogFolder
    description: The default folder where Spark logs will be written.
    types:
    - uid: string
  - name: properties.nodeSize
    description: The level of compute power that each node in the Big Data pool has.
    types:
    - uid: NodeSize
  - name: properties.nodeSizeFamily
    description: The kind of nodes that the Big Data pool provides.
    types:
    - uid: NodeSizeFamily
  - name: properties.lastSucceededTimestamp
    isReadyOnly: true
    description: The time when the Big Data pool was updated successfully.
    types:
    - uid: string
  - name: tags
    description: Resource tags.
    types:
    - uid: object
      isDictionary: true
      additionalTypes:
      - uid: string
      - uid: string
  - name: location
    description: The geo-location where the resource lives
    types:
    - uid: string
  - name: id
    isReadyOnly: true
    description: Fully qualified resource ID for the resource. Ex - /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}
    types:
    - uid: string
  - name: name
    isReadyOnly: true
    description: The name of the resource
    types:
    - uid: string
  - name: type
    isReadyOnly: true
    description: The type of the resource. E.g. "Microsoft.Compute/virtualMachines" or "Microsoft.Storage/storageAccounts"
    types:
    - uid: string
- name: ErrorResponse
  description: Error response
  kind: object
  properties:
  - name: error
    description: The error object.
    types:
    - uid: ErrorDetail
- name: ConfigurationType
  description: The type of the spark config properties file.
  kind: enum
  properties:
  - name: File
    types:
    - uid: string
  - name: Artifact
    types:
    - uid: string
- name: NodeSize
  description: The level of compute power that each node in the Big Data pool has.
  kind: enum
  properties:
  - name: None
    types:
    - uid: string
  - name: Small
    types:
    - uid: string
  - name: Medium
    types:
    - uid: string
  - name: Large
    types:
    - uid: string
  - name: XLarge
    types:
    - uid: string
  - name: XXLarge
    types:
    - uid: string
  - name: XXXLarge
    types:
    - uid: string
- name: NodeSizeFamily
  description: The kind of nodes that the Big Data pool provides.
  kind: enum
  properties:
  - name: None
    types:
    - uid: string
  - name: MemoryOptimized
    types:
    - uid: string
  - name: HardwareAcceleratedFPGA
    types:
    - uid: string
  - name: HardwareAcceleratedGPU
    types:
    - uid: string
- name: ErrorDetail
  description: The error detail.
  kind: object
  properties:
  - name: code
    isReadyOnly: true
    description: The error code.
    types:
    - uid: string
  - name: message
    isReadyOnly: true
    description: The error message.
    types:
    - uid: string
  - name: target
    isReadyOnly: true
    description: The error target.
    types:
    - uid: string
  - name: details
    isReadyOnly: true
    description: The error details.
    types:
    - uid: ErrorDetail
      isArray: true
  - name: additionalInfo
    isReadyOnly: true
    description: The error additional info.
    types:
    - uid: ErrorAdditionalInfo
      isArray: true
- name: ErrorAdditionalInfo
  description: The resource management error additional info.
  kind: object
  properties:
  - name: type
    isReadyOnly: true
    description: The additional info type.
    types:
    - uid: string
  - name: info
    isReadyOnly: true
    description: The additional info.
    types:
    - uid: object
examples:
- name: Create or update a Big Data pool
  request:
    uri: PUT https://management.azure.com/subscriptions/01234567-89ab-4def-0123-456789abcdef/resourceGroups/ExampleResourceGroup/providers/Microsoft.Synapse/workspaces/ExampleWorkspace/bigDataPools/ExamplePool?api-version=2021-06-01-preview
    body: >-
      {
        "tags": {
          "key": "value"
        },
        "location": "West US 2",
        "properties": {
          "sparkVersion": "2.4",
          "nodeCount": 4,
          "nodeSize": "Medium",
          "nodeSizeFamily": "MemoryOptimized",
          "autoScale": {
            "enabled": true,
            "minNodeCount": 3,
            "maxNodeCount": 50
          },
          "autoPause": {
            "enabled": true,
            "delayInMinutes": 15
          },
          "sparkEventsFolder": "/events",
          "libraryRequirements": {
            "content": "",
            "filename": "requirements.txt"
          },
          "defaultSparkLogFolder": "/logs"
        }
      }
    codeTab: |+
      # [HTTP](#tab/HTTP)
      ``` http
      PUT https://management.azure.com/subscriptions/01234567-89ab-4def-0123-456789abcdef/resourceGroups/ExampleResourceGroup/providers/Microsoft.Synapse/workspaces/ExampleWorkspace/bigDataPools/ExamplePool?api-version=2021-06-01-preview

      {
        "tags": {
          "key": "value"
        },
        "location": "West US 2",
        "properties": {
          "sparkVersion": "2.4",
          "nodeCount": 4,
          "nodeSize": "Medium",
          "nodeSizeFamily": "MemoryOptimized",
          "autoScale": {
            "enabled": true,
            "minNodeCount": 3,
            "maxNodeCount": 50
          },
          "autoPause": {
            "enabled": true,
            "delayInMinutes": 15
          },
          "sparkEventsFolder": "/events",
          "libraryRequirements": {
            "content": "",
            "filename": "requirements.txt"
          },
          "defaultSparkLogFolder": "/logs"
        }
      }

      ```

      # [Java](#tab/Java)
      ``` java
      import com.azure.resourcemanager.synapse.models.AutoPauseProperties;
      import com.azure.resourcemanager.synapse.models.AutoScaleProperties;
      import com.azure.resourcemanager.synapse.models.LibraryRequirements;
      import com.azure.resourcemanager.synapse.models.NodeSize;
      import com.azure.resourcemanager.synapse.models.NodeSizeFamily;
      import java.util.HashMap;
      import java.util.Map;

      /** Samples for BigDataPools CreateOrUpdate. */
      public final class Main {
          /*
           * x-ms-original-file: specification/synapse/resource-manager/Microsoft.Synapse/preview/2021-06-01-preview/examples/CreateOrUpdateBigDataPool.json
           */
          /**
           * Sample code: Create or update a Big Data pool.
           *
           * @param manager Entry point to SynapseManager.
           */
          public static void createOrUpdateABigDataPool(com.azure.resourcemanager.synapse.SynapseManager manager) {
              manager
                  .bigDataPools()
                  .define("ExamplePool")
                  .withRegion("West US 2")
                  .withExistingWorkspace("ExampleResourceGroup", "ExampleWorkspace")
                  .withTags(mapOf("key", "value"))
                  .withAutoScale(new AutoScaleProperties().withMinNodeCount(3).withEnabled(true).withMaxNodeCount(50))
                  .withAutoPause(new AutoPauseProperties().withDelayInMinutes(15).withEnabled(true))
                  .withSparkEventsFolder("/events")
                  .withNodeCount(4)
                  .withLibraryRequirements(new LibraryRequirements().withContent("").withFilename("requirements.txt"))
                  .withSparkVersion("2.4")
                  .withDefaultSparkLogFolder("/logs")
                  .withNodeSize(NodeSize.MEDIUM)
                  .withNodeSizeFamily(NodeSizeFamily.MEMORY_OPTIMIZED)
                  .create();
          }

          @SuppressWarnings("unchecked")
          private static <T> Map<String, T> mapOf(Object... inputs) {
              Map<String, T> map = new HashMap<>();
              for (int i = 0; i < inputs.length; i += 2) {
                  String key = (String) inputs[i];
                  T value = (T) inputs[i + 1];
                  map.put(key, value);
              }
              return map;
          }
      }

      ```
      Read this [SDK documentation](https://github.com/Azure/azure-sdk-for-java/blob/azure-resourcemanager-synapse_1.0.0-beta.6/sdk/synapse/azure-resourcemanager-synapse/README.md) on how to add the SDK to your project and authenticate.
      # [Go](#tab/Go)
      ``` go
      package armsynapse_test

      import (
      	"context"
      	"log"

      	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
      	"github.com/Azure/azure-sdk-for-go/sdk/azidentity"
      	"github.com/Azure/azure-sdk-for-go/sdk/resourcemanager/synapse/armsynapse"
      )

      // Generated from example definition: https://github.com/Azure/azure-rest-api-specs/tree/main/specification/synapse/resource-manager/Microsoft.Synapse/preview/2021-06-01-preview/examples/CreateOrUpdateBigDataPool.json
      func ExampleBigDataPoolsClient_BeginCreateOrUpdate() {
      	cred, err := azidentity.NewDefaultAzureCredential(nil)
      	if err != nil {
      		log.Fatalf("failed to obtain a credential: %v", err)
      	}
      	ctx := context.Background()
      	client, err := armsynapse.NewBigDataPoolsClient("01234567-89ab-4def-0123-456789abcdef", cred, nil)
      	if err != nil {
      		log.Fatalf("failed to create client: %v", err)
      	}
      	poller, err := client.BeginCreateOrUpdate(ctx,
      		"ExampleResourceGroup",
      		"ExampleWorkspace",
      		"ExamplePool",
      		armsynapse.BigDataPoolResourceInfo{
      			Location: to.Ptr("West US 2"),
      			Tags: map[string]*string{
      				"key": to.Ptr("value"),
      			},
      			Properties: &armsynapse.BigDataPoolResourceProperties{
      				AutoPause: &armsynapse.AutoPauseProperties{
      					DelayInMinutes: to.Ptr[int32](15),
      					Enabled:        to.Ptr(true),
      				},
      				AutoScale: &armsynapse.AutoScaleProperties{
      					Enabled:      to.Ptr(true),
      					MaxNodeCount: to.Ptr[int32](50),
      					MinNodeCount: to.Ptr[int32](3),
      				},
      				DefaultSparkLogFolder: to.Ptr("/logs"),
      				LibraryRequirements: &armsynapse.LibraryRequirements{
      					Content:  to.Ptr(""),
      					Filename: to.Ptr("requirements.txt"),
      				},
      				NodeCount:         to.Ptr[int32](4),
      				NodeSize:          to.Ptr(armsynapse.NodeSizeMedium),
      				NodeSizeFamily:    to.Ptr(armsynapse.NodeSizeFamilyMemoryOptimized),
      				SparkEventsFolder: to.Ptr("/events"),
      				SparkVersion:      to.Ptr("2.4"),
      			},
      		},
      		&armsynapse.BigDataPoolsClientBeginCreateOrUpdateOptions{Force: nil})
      	if err != nil {
      		log.Fatalf("failed to finish the request: %v", err)
      	}
      	res, err := poller.PollUntilDone(ctx, nil)
      	if err != nil {
      		log.Fatalf("failed to pull the result: %v", err)
      	}
      	// TODO: use response item
      	_ = res
      }

      ```
      Read this [SDK documentation](https://github.com/Azure/azure-sdk-for-go/blob/sdk%2Fresourcemanager%2Fsynapse%2Farmsynapse%2Fv0.5.0/sdk/resourcemanager/synapse/armsynapse/README.md) on how to add the SDK to your project and authenticate.
  responses:
  - statusCode: "200"
    body: >-
      {
        "id": "/subscriptions/01234567-89ab-4def-0123-456789abcdef/resourceGroups/ExampleResourceGroup/providers/Microsoft.Synapse/workspaces/ExampleWorkspace/bigDataPools/ExamplePool",
        "type": "Microsoft.Synapse/workspaces/bigDataPools",
        "location": "West US 2",
        "name": "ExamplePool",
        "tags": {
          "key": "value"
        },
        "properties": {
          "provisioningState": "Provisioning",
          "sparkVersion": "2.4",
          "nodeCount": 4,
          "nodeSize": "Medium",
          "nodeSizeFamily": "MemoryOptimized",
          "autoScale": {
            "enabled": true,
            "minNodeCount": 3,
            "maxNodeCount": 50
          },
          "autoPause": {
            "enabled": true,
            "delayInMinutes": 15
          },
          "creationDate": "1970-01-01T00:00:00Z",
          "sparkEventsFolder": "/events",
          "libraryRequirements": {
            "time": "1970-01-01T00:00:00Z",
            "content": "",
            "filename": "requirements.txt"
          },
          "defaultSparkLogFolder": "/logs",
          "lastSucceededTimestamp": "1970-01-01T10:00:00Z"
        }
      }
  - statusCode: "202"
    body: >-
      {
        "id": "/subscriptions/01234567-89ab-4def-0123-456789abcdef/resourceGroups/ExampleResourceGroup/providers/Microsoft.Synapse/workspaces/ExampleWorkspace/bigDataPools/ExamplePool",
        "type": "Microsoft.Synapse/workspaces/bigDataPools",
        "location": "West US 2",
        "name": "ExamplePool",
        "tags": {
          "key": "value"
        },
        "properties": {
          "provisioningState": "Provisioning",
          "sparkVersion": "2.4",
          "nodeCount": 4,
          "nodeSize": "Medium",
          "nodeSizeFamily": "MemoryOptimized",
          "autoScale": {
            "enabled": true,
            "minNodeCount": 3,
            "maxNodeCount": 50
          },
          "autoPause": {
            "enabled": true,
            "delayInMinutes": 15
          },
          "creationDate": "1970-01-01T00:00:00Z",
          "sparkEventsFolder": "/events",
          "libraryRequirements": {
            "time": "1970-01-01T00:00:00Z",
            "content": "",
            "filename": "requirements.txt"
          },
          "defaultSparkLogFolder": "/logs"
        }
      }
  - statusCode: default
    body: >-
      {
        "error": {
          "code": "Error code",
          "message": "Error message"
        }
      }
security: []
metadata:
  description: "Learn more about Synapse service - Create a Big Data pool.  \nCreate a new Big Data pool."
errorCodes: []
